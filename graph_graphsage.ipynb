{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 11:37:35.151565: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-24 11:37:35.228237: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-09-24 11:37:35.228254: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-09-24 11:37:35.684616: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-09-24 11:37:35.684677: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-09-24 11:37:35.684683: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-09-24 11:37:36.092431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-24 11:37:36.092654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-09-24 11:37:36.092856: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-09-24 11:37:36.092907: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2024-09-24 11:37:36.092950: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2024-09-24 11:37:36.092991: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2024-09-24 11:37:36.124969: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2024-09-24 11:37:36.125019: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2024-09-24 11:37:36.125027: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-09-24 11:37:36.125332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph.mapper import GraphSAGENodeGenerator\n",
    "from stellargraph.layer import GraphSAGE\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "df = pd.read_csv('imdb_ds_2k_clean.csv')\n",
    "#df = df[:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento de texto\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([char for char in text if char.isalnum() or char == ' '])\n",
    "    return text\n",
    "\n",
    "df['clean_review'] = df['sw_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Crear matriz TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_tfidf = tfidf.fit_transform(df['clean_review'])  # Matriz de tamaño (2000, 1000)\n",
    "\n",
    "terms = tfidf.get_feature_names_out()  # Lista de términos, tamaño 1000\n",
    "\n",
    "# Obtener la matriz término-documento\n",
    "X_td = X_tfidf.T  # Matriz término-documento de tamaño (1000, 2000)\n",
    "\n",
    "# Reducir la dimensionalidad de las características de los nodos\n",
    "pca = PCA(n_components=100)\n",
    "X_reduced = pca.fit_transform(X_td.toarray())  # Resultado de tamaño (1000, 100)\n",
    "\n",
    "# Crear un DataFrame de términos y sus índices\n",
    "terms_df = pd.DataFrame({'term': terms})\n",
    "terms_df['term_id'] = terms_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear grafo\n",
    "G_nx = nx.Graph()\n",
    "\n",
    "# Añadir nodos al grafo con sus características (vectores reducidos)\n",
    "for idx, term in enumerate(terms):\n",
    "    feature_vector = X_reduced[idx]  # Vector de tamaño (100,)\n",
    "    G_nx.add_node(term, features=feature_vector)\n",
    "\n",
    "# Añadir aristas basadas en co-ocurrencias en los documentos\n",
    "for doc in X_tfidf.toarray():\n",
    "    # Obtener índices de términos con peso mayor a 0\n",
    "    non_zero_indices = np.where(doc > 0)[0]\n",
    "    non_zero_terms = [terms[idx] for idx in non_zero_indices]\n",
    "    \n",
    "    # Añadir aristas entre términos que co-ocurren\n",
    "    for i, term1 in enumerate(non_zero_terms):\n",
    "        for term2 in non_zero_terms[i+1:]:\n",
    "            if G_nx.has_edge(term1, term2):\n",
    "                G_nx[term1][term2]['weight'] += 1\n",
    "            else:\n",
    "                G_nx.add_edge(term1, term2, weight=1)\n",
    "\n",
    "# Convertir el grafo de NetworkX a StellarGraph\n",
    "G = StellarGraph.from_networkx(G_nx, node_features='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar los datos para GraphSAGE\n",
    "nodes = list(G.nodes())\n",
    "node_features = G.node_features(nodes)\n",
    "\n",
    "# Dividir los nodos en entrenamiento y prueba, junto con sus características\n",
    "train_nodes, test_nodes, train_targets, test_targets = train_test_split(\n",
    "    nodes, node_features, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el generador de nodos\n",
    "batch_size = 50\n",
    "num_samples = [40, 40] #numero de caminos en primerca y segunda capa\n",
    "generator = GraphSAGENodeGenerator(G, batch_size, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Crear generadores de entrenamiento y prueba con targets\n",
    "train_gen = generator.flow(train_nodes, targets=train_targets, shuffle=True)\n",
    "test_gen = generator.flow(test_nodes, targets=test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymamani/anaconda3/envs/virtual3.7/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  f\"The initializer {self.__class__.__name__} is unseeded \"\n",
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 3s 122ms/step - loss: 0.0091 - val_loss: 0.0074\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0074 - val_loss: 0.0068\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0068 - val_loss: 0.0065\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0066 - val_loss: 0.0064\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0064 - val_loss: 0.0062\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0062 - val_loss: 0.0062\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0061 - val_loss: 0.0062\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0062 - val_loss: 0.0061\n"
     ]
    }
   ],
   "source": [
    "# Definir el modelo de GraphSAGE\n",
    "layer_sizes = [50, 300]\n",
    "graphsage = GraphSAGE(\n",
    "    layer_sizes=layer_sizes, generator=generator, bias=True, dropout=0.3\n",
    ")\n",
    "\n",
    "# Construir el modelo\n",
    "x_inp, x_out = graphsage.in_out_tensors()\n",
    "prediction = tf.keras.layers.Dense(units=100, activation='linear')(x_out)  # Salida de dimensión 100\n",
    "\n",
    "# Obtener las representaciones de los nodos (embeddings)\n",
    "model = Model(inputs=x_inp, outputs=prediction)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    loss='mse',  # Usamos MSE para reconstruir las características\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=test_gen,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    use_multiprocessing=False,\n",
    "    workers=1,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=2)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 2s 80ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Obtener las representaciones de los nodos\n",
    "embedding_model = Model(inputs=x_inp, outputs=x_out)  # x_out es el embedding\n",
    "node_embeddings = embedding_model.predict(generator.flow(nodes), verbose=1)\n",
    "\n",
    "# Crear un diccionario de embeddings de nodos\n",
    "node_embeddings_dict = dict(zip(nodes, node_embeddings))\n",
    "\n",
    "\n",
    "# Guardar los embeddings en un archivo .txt\n",
    "with open('embedding_imdb2k_gsage_le40nw300.txt', 'w') as f:\n",
    "    # Escribir la cantidad de nodos y el tamaño del embedding en la primera línea\n",
    "    f.write(f\"{len(node_embeddings_dict)} {layer_sizes[-1]}\\n\")\n",
    "    \n",
    "    # Escribir cada nodo y su embedding en las líneas siguientes\n",
    "    for node, embedding in node_embeddings_dict.items():\n",
    "        embedding_str = ' '.join(map(str, embedding))\n",
    "        f.write(f\"{node} {embedding_str}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Función para obtener el embedding de un documento\n",
    "def get_doc_embedding(doc):\n",
    "    words = doc.split()\n",
    "    embeddings = [node_embeddings_dict[word] for word in words if word in node_embeddings_dict]\n",
    "    if embeddings:\n",
    "        embedding = np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        embedding = np.zeros(layer_sizes[-1])\n",
    "    return embedding\n",
    "\n",
    "df['embedding'] = df['clean_review'].apply(get_doc_embedding)\n",
    "\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "X = np.vstack(df['embedding'])\n",
    "y = df['sentiment']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(true_labels, predictions, model_name):\n",
    "    acc = accuracy_score(true_labels, predictions)\n",
    "    prec = precision_score(true_labels, predictions)\n",
    "    rec = recall_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    \n",
    "    print(f\"Resultados del modelo {model_name}:\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_predictions = svm_model.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "rf_model3 = XGBClassifier()\n",
    "rf_model3.fit(X_train, y_train)\n",
    "rf_predictions3 = rf_model3.predict(X_test)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "rf_model4 = KNeighborsClassifier(n_neighbors=5)\n",
    "rf_model4.fit(X_train, y_train)\n",
    "rf_predictions4 = rf_model4.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados del modelo SVM:\n",
      "Accuracy: 0.6500\n",
      "Precision: 0.6498\n",
      "Recall: 0.6878\n",
      "F1-Score: 0.6682\n",
      "\n",
      "Resultados del modelo Random Forest:\n",
      "Accuracy: 0.7150\n",
      "Precision: 0.7358\n",
      "Recall: 0.6927\n",
      "F1-Score: 0.7136\n",
      "\n",
      "Resultados del modelo XGBoots:\n",
      "Accuracy: 0.7100\n",
      "Precision: 0.7259\n",
      "Recall: 0.6976\n",
      "F1-Score: 0.7114\n",
      "\n",
      "Resultados del modelo KNN:\n",
      "Accuracy: 0.6175\n",
      "Precision: 0.6444\n",
      "Recall: 0.5659\n",
      "F1-Score: 0.6026\n",
      "\n",
      "Resultados del modelo LR:\n",
      "Accuracy: 0.6425\n",
      "Precision: 0.6520\n",
      "Recall: 0.6488\n",
      "F1-Score: 0.6504\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(y_test, svm_predictions, \"SVM\")\n",
    "evaluate_model(y_test, rf_predictions, \"Random Forest\")\n",
    "evaluate_model(y_test, rf_predictions3, \"XGBoots\")\n",
    "evaluate_model(y_test, rf_predictions4, \"KNN\")\n",
    "evaluate_model(y_test, y_pred, \"LR\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
