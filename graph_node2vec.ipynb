{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymamani/anaconda3/envs/virtual3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from node2vec import Node2Vec\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sw_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewer ha mention watch oz episode youll...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production film technique una...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>think wa wonderful way spend time hot summer w...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter matteis love time money visually stun f...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>feel minnesota direct steven baigelmann star k...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>cell rat cell like antz must watch twice appre...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>movie despite list list celebs complete waste ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>love movie wa could break tear watch really up...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>wa worst movie ever see billy zane understand ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sw_text sentiment\n",
       "0     one reviewer ha mention watch oz episode youll...  positive\n",
       "1     wonderful little production film technique una...  positive\n",
       "2     think wa wonderful way spend time hot summer w...  positive\n",
       "3     basically family little boy jake think zombie ...  negative\n",
       "4     petter matteis love time money visually stun f...  positive\n",
       "...                                                 ...       ...\n",
       "1995  feel minnesota direct steven baigelmann star k...  negative\n",
       "1996  cell rat cell like antz must watch twice appre...  positive\n",
       "1997  movie despite list list celebs complete waste ...  negative\n",
       "1998  love movie wa could break tear watch really up...  positive\n",
       "1999  wa worst movie ever see billy zane understand ...  negative\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar datos de IMDB\n",
    "# Asegúrate de tener un dataframe con columnas \"review\" y \"sentiment\" (1 = positivo, 0 = negativo)\n",
    "df = pd.read_csv('imdb_ds_2k_clean.csv')  # Cambia por la ruta correcta\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento de texto\n",
    "def preprocess_text(text):\n",
    "    # Limpieza básica (minúsculas, quitar puntuación, etc.)\n",
    "    text = text.lower()\n",
    "    text = ''.join([char for char in text if char.isalnum() or char == ' '])\n",
    "    return text\n",
    "\n",
    "df['clean_review'] = df['sw_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear grafo basado en co-ocurrencias de términos\n",
    "tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_tfidf = tfidf.fit_transform(df['clean_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "G = nx.Graph()\n",
    "terms = tfidf.get_feature_names_out()\n",
    "\n",
    "# Añadir nodos y conexiones al grafo optimizado\n",
    "for doc in X_tfidf.toarray():\n",
    "    # Obtener índices de términos con peso mayor a 0\n",
    "    non_zero_indices = np.where(doc > 0)[0]\n",
    "    non_zero_terms = [(terms[idx], doc[idx]) for idx in non_zero_indices]\n",
    "    \n",
    "    # Añadir nodos\n",
    "    for term, weight in non_zero_terms:\n",
    "        G.add_node(term)\n",
    "    \n",
    "    # Añadir aristas con pesos\n",
    "    for i, (term1, weight1) in enumerate(non_zero_terms):\n",
    "        for term2, weight2 in non_zero_terms[i+1:]:\n",
    "            G.add_edge(term1, term2, weight=weight1 * weight2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 1000/1000 [18:25<00:00,  1.11s/it]\n",
      "Generating walks (CPU: 1): 100%|██████████| 11/11 [00:21<00:00,  2.00s/it]\n",
      "Generating walks (CPU: 2): 100%|██████████| 11/11 [00:21<00:00,  1.95s/it]\n",
      "Generating walks (CPU: 3): 100%|██████████| 11/11 [00:21<00:00,  1.98s/it]\n",
      "Generating walks (CPU: 4): 100%|██████████| 11/11 [00:22<00:00,  2.04s/it]\n",
      "Generating walks (CPU: 5): 100%|██████████| 11/11 [00:22<00:00,  2.01s/it]\n",
      "Generating walks (CPU: 6): 100%|██████████| 11/11 [00:22<00:00,  2.03s/it]\n",
      "Generating walks (CPU: 7): 100%|██████████| 11/11 [00:22<00:00,  2.04s/it]\n",
      "Generating walks (CPU: 8): 100%|██████████| 11/11 [00:22<00:00,  2.05s/it]\n",
      "Generating walks (CPU: 9): 100%|██████████| 11/11 [00:21<00:00,  1.96s/it]\n",
      "Generating walks (CPU: 10): 100%|██████████| 11/11 [00:22<00:00,  2.03s/it]\n",
      "Generating walks (CPU: 11): 100%|██████████| 11/11 [00:21<00:00,  1.98s/it]\n",
      "Generating walks (CPU: 12): 100%|██████████| 11/11 [00:22<00:00,  2.06s/it]\n",
      "Generating walks (CPU: 13): 100%|██████████| 11/11 [00:22<00:00,  2.02s/it]\n",
      "Generating walks (CPU: 14): 100%|██████████| 11/11 [00:22<00:00,  2.02s/it]\n",
      "Generating walks (CPU: 15): 100%|██████████| 11/11 [00:22<00:00,  2.03s/it]\n",
      "Generating walks (CPU: 16): 100%|██████████| 11/11 [00:21<00:00,  1.97s/it]\n",
      "Generating walks (CPU: 17): 100%|██████████| 11/11 [00:21<00:00,  1.99s/it]\n",
      "Generating walks (CPU: 18): 100%|██████████| 11/11 [00:22<00:00,  2.00s/it]\n",
      "Generating walks (CPU: 19): 100%|██████████| 11/11 [00:21<00:00,  1.97s/it]\n",
      "Generating walks (CPU: 20): 100%|██████████| 11/11 [00:22<00:00,  2.01s/it]\n",
      "Generating walks (CPU: 21): 100%|██████████| 10/10 [00:19<00:00,  1.97s/it]\n",
      "Generating walks (CPU: 22): 100%|██████████| 10/10 [00:19<00:00,  1.97s/it]\n",
      "Generating walks (CPU: 23): 100%|██████████| 10/10 [00:19<00:00,  1.99s/it]\n",
      "Generating walks (CPU: 24): 100%|██████████| 10/10 [00:19<00:00,  1.96s/it]\n",
      "Generating walks (CPU: 25): 100%|██████████| 10/10 [00:19<00:00,  1.98s/it]\n",
      "Generating walks (CPU: 26): 100%|██████████| 10/10 [00:19<00:00,  1.99s/it]\n",
      "Generating walks (CPU: 27): 100%|██████████| 10/10 [00:19<00:00,  1.99s/it]\n",
      "Generating walks (CPU: 28): 100%|██████████| 10/10 [00:19<00:00,  1.98s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Aplicar Node2Vec para generar embeddings\n",
    "node2vec = Node2Vec(G, dimensions=300, walk_length=40, num_walks=300, workers=28)\n",
    "model = node2vec.fit(window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abre un archivo en modo escritura\n",
    "with open('embedding_imdb2k_n2v_w5d300_le40nw300.txt', 'w') as f:\n",
    "    # Escribe la cantidad de palabras y las dimensiones\n",
    "    f.write(f\"{len(model.wv.vectors)} {model.wv.vector_size}\\n\")\n",
    "    \n",
    "    # Itera sobre cada palabra y sus embeddings\n",
    "    for word in model.wv.index_to_key:\n",
    "        # Obtén los embeddings de la palabra\n",
    "        vector = model.wv[word]\n",
    "        # Escribe la palabra seguida de sus valores de embedding\n",
    "        f.write(f\"{word} {' '.join(map(str, vector))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2000 records to imdb\n",
    "#18 min probability calculate\n",
    "#8 min walks\n",
    "\n",
    "# Obtener embeddings para cada documento\n",
    "def get_doc_embedding(doc):\n",
    "    words = doc.split()\n",
    "    embedding = np.mean([model.wv[word] for word in words if word in model.wv], axis=0)\n",
    "    return embedding if embedding.size else np.zeros(300)\n",
    "\n",
    "df['embedding'] = df['clean_review'].apply(get_doc_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para clasificación\n",
    "X = np.vstack(df['embedding'])\n",
    "y = df['sentiment']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(true_labels, predictions, model_name):\n",
    "    acc = accuracy_score(true_labels, predictions)\n",
    "    prec = precision_score(true_labels, predictions)\n",
    "    rec = recall_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    \n",
    "    print(f\"Resultados del modelo {model_name}:\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_predictions = svm_model.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "rf_model3 = XGBClassifier()\n",
    "rf_model3.fit(X_train, y_train)\n",
    "rf_predictions3 = rf_model3.predict(X_test)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "rf_model4 = KNeighborsClassifier(n_neighbors=5)\n",
    "rf_model4.fit(X_train, y_train)\n",
    "rf_predictions4 = rf_model4.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados del modelo SVM:\n",
      "Accuracy: 0.8075\n",
      "Precision: 0.8019\n",
      "Recall: 0.8293\n",
      "F1-Score: 0.8153\n",
      "\n",
      "Resultados del modelo Random Forest:\n",
      "Accuracy: 0.7075\n",
      "Precision: 0.7115\n",
      "Recall: 0.7220\n",
      "F1-Score: 0.7167\n",
      "\n",
      "Resultados del modelo XGBoots:\n",
      "Accuracy: 0.7700\n",
      "Precision: 0.7783\n",
      "Recall: 0.7707\n",
      "F1-Score: 0.7745\n",
      "\n",
      "Resultados del modelo KNN:\n",
      "Accuracy: 0.6325\n",
      "Precision: 0.6686\n",
      "Recall: 0.5610\n",
      "F1-Score: 0.6101\n",
      "\n",
      "Resultados del modelo LR:\n",
      "Accuracy: 0.7925\n",
      "Precision: 0.8020\n",
      "Recall: 0.7902\n",
      "F1-Score: 0.7961\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(y_test, svm_predictions, \"SVM\")\n",
    "evaluate_model(y_test, rf_predictions, \"Random Forest\")\n",
    "evaluate_model(y_test, rf_predictions3, \"XGBoots\")\n",
    "evaluate_model(y_test, rf_predictions4, \"KNN\")\n",
    "evaluate_model(y_test, y_pred, \"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
