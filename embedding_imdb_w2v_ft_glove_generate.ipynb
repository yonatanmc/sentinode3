{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha y Hora actual Inicio: 2024-05-20 11:17:22.567120\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "fecha_hora_actual = datetime.now()\n",
    "print(\"Fecha y Hora actual Inicio:\", fecha_hora_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ymamani/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ymamani/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ymamani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.parsing.preprocessing import (\n",
    "    strip_punctuation,\n",
    "    strip_numeric,\n",
    "    strip_short,\n",
    "    stem_text,\n",
    "    strip_multiple_whitespaces,\n",
    "    remove_stopwords,\n",
    "    STOPWORDS,\n",
    ")\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvis.network import Network\n",
    "import time\n",
    "\n",
    "# ejecutar dataframe de forma paralela\n",
    "from pandarallel import pandarallel  # import pandarallel\n",
    "\n",
    "pandarallel.initialize()  # initialize pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sw_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewer ha mention watch oz episode youll...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production film technique una...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>think wa wonderful way spend time hot summer w...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter matteis love time money visually stun f...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>feel minnesota direct steven baigelmann star k...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>cell rat cell like antz must watch twice appre...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>movie despite list list celebs complete waste ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>love movie wa could break tear watch really up...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>wa worst movie ever see billy zane understand ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sw_text sentiment\n",
       "0     one reviewer ha mention watch oz episode youll...  positive\n",
       "1     wonderful little production film technique una...  positive\n",
       "2     think wa wonderful way spend time hot summer w...  positive\n",
       "3     basically family little boy jake think zombie ...  negative\n",
       "4     petter matteis love time money visually stun f...  positive\n",
       "...                                                 ...       ...\n",
       "1995  feel minnesota direct steven baigelmann star k...  negative\n",
       "1996  cell rat cell like antz must watch twice appre...  positive\n",
       "1997  movie despite list list celebs complete waste ...  negative\n",
       "1998  love movie wa could break tear watch really up...  positive\n",
       "1999  wa worst movie ever see billy zane understand ...  negative\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ds_imdb = pd.read_csv('imdb_ds_2k_clean.csv')\n",
    "ds_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de dataset (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape de dataset\", ds_imdb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realizando tokenizacion de palabras ...\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "# tokenización de palabras\n",
    "print(\"Realizando tokenizacion de palabras ...\")\n",
    "ds_imdb[\"token_text\"] = [word_tokenize(text) for text in ds_imdb[\"sw_text\"]]\n",
    "print(len(ds_imdb[\"token_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de palabras unicas: 25016\n"
     ]
    }
   ],
   "source": [
    "# Obtener palabras unicas de todas las oraciones\n",
    "all_words_ds_sentiment = list(set(word for text in ds_imdb[\"token_text\"] for word in text))\n",
    "print(\"Total de palabras unicas:\", len(all_words_ds_sentiment))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_array = ds_imdb['sw_text'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracion_tokens = []\n",
    "for oracion in ds_array:\n",
    "    # Eliminar puntuación y dividir por espacios\n",
    "    tokens = oracion.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    # Convertir a minúsculas\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    if tokens:  # Añadir solo si hay tokens\n",
    "        oracion_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "def word_embeddings_w2v(imdb_sentences, window_size, embedding_vector_size, num_workers):\n",
    "    model = Word2Vec(\n",
    "        sentences=imdb_sentences,\n",
    "        window=window_size,\n",
    "        vector_size=embedding_vector_size,\n",
    "        sg=1,\n",
    "        workers=num_workers,\n",
    "        min_count=1\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando vectorizacion de palabras ...\n",
      "Tiempo de ejecución vectorizacion de palabras Skip-gram W2V: 0.0004 horas\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "tiempo_inicio = time.time()\n",
    "print(\"Generando vectorizacion de palabras ...\")\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "word_emb_wv = word_embeddings_w2v(oracion_tokens, 5, 300, num_cores)\n",
    "\n",
    "tiempo_final = time.time()\n",
    "tiempo_ejecucion_horas = (tiempo_final - tiempo_inicio) / 3600\n",
    "print(f\"Tiempo de ejecución vectorizacion de palabras Skip-gram W2V: {tiempo_ejecucion_horas:.4f} horas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando embeddings W2V en archivo txt ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Guardando embeddings W2V en archivo txt ...\")\n",
    "#word_emb_wv.wv.save_word2vec_format(\"embedding_imdb2k_w2v.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "def word_embeddings_ft(imdb_sentences, window_size, embedding_vector_size, num_workers):\n",
    "    model = FastText(\n",
    "        sentences=imdb_sentences,\n",
    "        window=window_size,\n",
    "        vector_size=embedding_vector_size,   \n",
    "        sg=1,     \n",
    "        workers=num_workers,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando vectorizacion de palabras FastText ...\n",
      "Tiempo de ejecución vectorizacion de palabras Skip-gram FT: 0.0009 horas\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "tiempo_inicio = time.time()\n",
    "print(\"Generando vectorizacion de palabras FastText ...\")\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "word_emb_ft = word_embeddings_ft(oracion_tokens, 5, 300, num_cores)\n",
    "\n",
    "tiempo_final = time.time()\n",
    "tiempo_ejecucion_horas = (tiempo_final - tiempo_inicio) / 3600\n",
    "print(f\"Tiempo de ejecución vectorizacion de palabras Skip-gram FT: {tiempo_ejecucion_horas:.4f} horas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando embeddings FT en archivo txt ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Guardando embeddings FT en archivo txt ...\")\n",
    "#word_emb_ft.wv.save_word2vec_format(\"embedding_imdb2k_ft.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'glove'...\n",
      "remote: Enumerating objects: 656, done.\u001b[K\n",
      "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
      "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
      "remote: Total 656 (delta 36), reused 47 (delta 32), pack-reused 592 (from 1)\u001b[K\n",
      "Receiving objects: 100% (656/656), 245.96 KiB | 375.00 KiB/s, done.\n",
      "Resolving deltas: 100% (374/374), done.\n",
      "mkdir -p build\n",
      "gcc -c src/vocab_count.c -o build/vocab_count.o -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "gcc -c src/cooccur.c -o build/cooccur.o -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmerge_files\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:180:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  180 |         \u001b[01;35m\u001b[Kfread(&new, sizeof(CREC), 1, fid[i])\u001b[m\u001b[K;\n",
      "      |         \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:190:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  190 |     \u001b[01;35m\u001b[Kfread(&new, sizeof(CREC), 1, fid[i])\u001b[m\u001b[K;\n",
      "      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:203:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  203 |         \u001b[01;35m\u001b[Kfread(&new, sizeof(CREC), 1, fid[i])\u001b[m\u001b[K;\n",
      "      |         \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "gcc -c src/shuffle.c -o build/shuffle.o -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "\u001b[01m\u001b[Ksrc/shuffle.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kshuffle_merge\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/shuffle.c:96:17:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "   96 |                 \u001b[01;35m\u001b[Kfread(&array[i], sizeof(CREC), 1, fid[j])\u001b[m\u001b[K;\n",
      "      |                 \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/shuffle.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kshuffle_by_chunks\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/shuffle.c:161:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  161 |         \u001b[01;35m\u001b[Kfread(&array[i], sizeof(CREC), 1, fin)\u001b[m\u001b[K;\n",
      "      |         \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "gcc -c src/glove.c -o build/glove.o -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "\u001b[01m\u001b[Ksrc/glove.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kload_init_file\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/glove.c:86:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "   86 |         \u001b[01;35m\u001b[Kfread(&array[a], sizeof(real), 1, fin)\u001b[m\u001b[K;\n",
      "      |         \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kglove_thread\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/glove.c:182:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      "  182 |         \u001b[01;35m\u001b[Kfread(&cr, sizeof(CREC), 1, fin)\u001b[m\u001b[K;\n",
      "      |         \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "gcc -c src/common.c -o build/common.o -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "gcc build/vocab_count.o build/common.o -o build/vocab_count -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "gcc build/cooccur.o build/common.o -o build/cooccur -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "gcc build/shuffle.o build/common.o -o build/shuffle -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "gcc build/glove.o build/common.o -o build/glove -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/stanfordnlp/glove\n",
    "! cd glove && make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path\n",
    "#import os\n",
    "#NLP_REPO_PATH = '/home/ymamani/projects/code/embeddings_w2v_ft_glove/data'\n",
    "#glove_model_path = os.path.join(NLP_REPO_PATH, \"utils_nlp\", \"models\", \"glove\")\n",
    "\n",
    "# Execute shell commands\n",
    "#!cd $glove_model_path && make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#SAVE_FILES_PATH = '/home/ymamani/projects/code/embeddings_w2v_ft_glove/data/trained_word_embeddings'\n",
    "SAVE_FILES_PATH = '/home/ymamani/projects/code/experimentos3/data/trained_word_embeddings'\n",
    "# Save our corpus as tokens delimited by spaces with new line characters in between sentences.\n",
    "training_corpus_file_path = os.path.join(SAVE_FILES_PATH, \"training-corpus-cleaned.txt\")\n",
    "with open(training_corpus_file_path, 'w', encoding='utf8') as file:\n",
    "    for sent in oracion_tokens:\n",
    "        file.write(\" \".join(sent) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING VOCABULARY\n",
      "Processed 0 tokens.\u001b[11G100000 tokens.\u001b[11G200000 tokens.\u001b[0GProcessed 236760 tokens.\n",
      "Counted 25016 unique words.\n",
      "Truncating vocabulary at min count 5.\n",
      "Using vocabulary of size 5391.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define path\n",
    "#glove_model_path = '/home/ymamani/projects/code/embeddings_w2v_ft_glove/glove'\n",
    "glove_model_path = '/home/ymamani/projects/code/experimentos3/glove'\n",
    "vocab_count_exe_path = os.path.join(glove_model_path, \"build\", \"vocab_count\")\n",
    "vocab_file_path = os.path.join(SAVE_FILES_PATH, \"vocab.txt\")\n",
    "# Execute shell commands\n",
    "!$vocab_count_exe_path -min-count 5 -verbose 2 <$training_corpus_file_path> $vocab_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNTING COOCCURRENCES\n",
      "window size: 5\n",
      "context: symmetric\n",
      "max product: 98356909\n",
      "overflow length: 304226850\n",
      "Reading vocab from file \"/home/ymamani/projects/code/experimentos3/data/trained_word_embeddings/vocab.txt\"...loaded 5391 words.\n",
      "Building lookup table...table contains 29062882 elements.\n",
      "Processing token: 0\u001b[19G100000\u001b[19G200000\u001b[0GProcessed 236760 tokens.\n",
      "Writing cooccurrences to disk.......2 files in total.\n",
      "Merging cooccurrence files: processed 0 lines.\u001b[39G0 lines.\u001b[39G100000 lines.\u001b[39G200000 lines.\u001b[39G300000 lines.\u001b[39G400000 lines.\u001b[39G500000 lines.\u001b[39G600000 lines.\u001b[39G700000 lines.\u001b[39G800000 lines.\u001b[39G900000 lines.\u001b[39G1000000 lines.\u001b[0GMerging cooccurrence files: processed 1093837 lines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define path\n",
    "cooccur_exe_path = os.path.join(glove_model_path, \"build\", \"cooccur\")\n",
    "cooccurrence_file_path = os.path.join(SAVE_FILES_PATH, \"cooccurrence.bin\")\n",
    "# Execute shell commands\n",
    "!$cooccur_exe_path -memory 32 -vocab-file $vocab_file_path -verbose 2 -window-size 5 <$training_corpus_file_path> $cooccurrence_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random seed 1726766747\n",
      "SHUFFLING COOCCURRENCES\n",
      "array size: 2040109465\n",
      "Shuffling by chunks: processed 0 lines.\u001b[22Gprocessed 1093837 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.\u001b[31G1093837 lines.\u001b[0GMerging temp files: processed 1093837 lines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define path\n",
    "shuffle_exe_path = os.path.join(glove_model_path, \"build\", \"shuffle\")\n",
    "cooccurrence_shuf_file_path = os.path.join(SAVE_FILES_PATH, \"cooccurrence.shuf.bin\")\n",
    "# Execute shell commands\n",
    "!$shuffle_exe_path -memory 32 -verbose 2 <$cooccurrence_file_path> $cooccurrence_shuf_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL\n",
      "Read 1093837 lines.\n",
      "Initializing parameters...Using random seed 1726766753\n",
      "done.\n",
      "vector size: 300\n",
      "vocab size: 5391\n",
      "x_max: 10.000000\n",
      "alpha: 0.750000\n",
      "09/19/24 - 12:25.54PM, iter: 001, cost: 0.055437\n",
      "09/19/24 - 12:25.54PM, iter: 002, cost: 0.047176\n",
      "09/19/24 - 12:25.55PM, iter: 003, cost: 0.039700\n",
      "09/19/24 - 12:25.56PM, iter: 004, cost: 0.038113\n",
      "09/19/24 - 12:25.56PM, iter: 005, cost: 0.037385\n",
      "09/19/24 - 12:25.57PM, iter: 006, cost: 0.036717\n",
      "09/19/24 - 12:25.58PM, iter: 007, cost: 0.035874\n",
      "09/19/24 - 12:25.59PM, iter: 008, cost: 0.034901\n",
      "09/19/24 - 12:25.59PM, iter: 009, cost: 0.033797\n",
      "09/19/24 - 12:26.00PM, iter: 010, cost: 0.032539\n",
      "09/19/24 - 12:26.01PM, iter: 011, cost: 0.031166\n",
      "09/19/24 - 12:26.01PM, iter: 012, cost: 0.029687\n",
      "09/19/24 - 12:26.02PM, iter: 013, cost: 0.028176\n",
      "09/19/24 - 12:26.03PM, iter: 014, cost: 0.026693\n",
      "09/19/24 - 12:26.03PM, iter: 015, cost: 0.025256\n"
     ]
    }
   ],
   "source": [
    "# Define path\n",
    "glove_exe_path = os.path.join(glove_model_path, \"build\", \"glove\")\n",
    "glove_vector_file_path = os.path.join(SAVE_FILES_PATH, \"GloVe_vectors\")\n",
    "# Execute shell commands\n",
    "!$glove_exe_path -save-file $glove_vector_file_path -threads 8 -input-file $cooccurrence_shuf_file_path -x-max 10 -iter 15 -vector-size 300 -binary 2 -vocab-file $vocab_file_path -verbose 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the saved word vectors.\n",
    "glove_wv = {}\n",
    "glove_vector_txt_file_path = os.path.join(SAVE_FILES_PATH, \"GloVe_vectors.txt\")\n",
    "with open(glove_vector_txt_file_path, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        split_line = line.split(\" \")\n",
    "        glove_wv[split_line[0]] = [float(i) for i in split_line[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for apple: [-0.235367, -0.194131, -0.337478, 0.069839, -0.525106, -1.209318, -0.125127, 0.380385, 0.204849, 0.337658, 0.786869, 0.045477, -0.271602, 0.398011, -1.151463, 0.350926, -0.241669, 0.158509, -1.120875, 0.695348, 0.099997, -0.706794, 0.036722, -0.225602, 0.35696, 0.161608, -0.376903, 0.03142, 0.542876, -0.447508, 0.356882, 0.825494, -0.196084, 0.09937, -0.480333, 0.035598, 0.989132, -0.073479, -0.007345, -0.095263, 0.30634, -0.322474, 0.277669, 0.160892, -0.084347, -0.641337, -0.400511, -0.027045, -0.356613, -0.418473, 0.485058, -1.056487, -0.082592, -0.1274, 0.043072, -0.071779, -0.008721, 0.251047, 0.414428, -0.783374, 0.573423, -0.375085, 0.416375, -0.029831, 0.041344, -0.406137, -0.53061, -0.004196, 0.010236, 0.100226, -0.398907, 0.369308, -0.407135, 0.142113, -0.563081, 0.829507, 0.282536, 0.166587, -0.431276, 0.154563, 0.112659, -0.925348, 0.031795, -0.565388, 0.568706, 0.024221, 0.251521, 0.059045, -0.501073, 0.323, 0.004241, 0.30676, -0.426005, 0.57374, 0.128327, -0.067352, 0.324345, -0.02909, -0.214351, 0.630801, -0.602345, 0.603727, 0.072651, -0.572601, 0.48899, 0.2917, -0.688796, 0.051329, 0.418803, 0.14176, 0.430259, 0.205173, 0.656296, 0.021234, -0.333158, 0.03612, 0.720391, -0.454755, -0.203369, 0.305429, 0.687918, -0.513362, -0.042695, -0.012647, -0.535319, 0.537167, -0.05428, 0.128031, 0.185858, 0.199558, 0.830285, 0.383053, 0.437018, 0.632183, -0.703639, -0.333503, 0.675681, 0.847291, -0.413962, -0.949892, 0.286959, -0.441733, 0.456067, -0.213006, 0.787871, 0.78131, -0.018802, 0.018589, 0.422925, 0.145175, -0.054049, 0.397463, 0.007695, 0.516945, 0.251209, 0.731404, -0.243842, -0.354203, 0.743928, 0.95334, 0.168926, 0.118198, -0.257413, -0.250913, 0.273988, 0.386571, -0.119909, -0.225784, -0.095618, -0.262465, -0.134818, 0.657729, 1.336578, -0.213866, 0.102268, -0.099618, -0.285591, -0.612828, 0.250747, -0.201826, -0.464944, 0.389865, -0.334143, 0.070693, -0.807157, -0.188187, 0.610686, 0.236451, 0.14756, 0.363616, -1.246705, 0.215648, 0.558443, -0.293378, 0.078488, 0.278258, -0.320144, -0.119632, 0.162034, -0.286932, 0.208232, 0.917766, 0.441203, -0.733781, 0.329612, 0.554925, -0.26362, 0.085927, 0.447679, 0.784121, -0.756416, 0.308007, -0.027611, -0.086035, 0.85301, 0.206578, -0.157648, 0.0779, 0.247592, -1.023398, -0.032155, 0.1843, -0.475485, 0.189456, -0.093575, 0.431381, -0.172949, -0.398415, 0.442514, 0.450664, 0.199277, -0.413823, -0.297091, -0.306422, -0.090195, 0.742195, -0.217716, -0.070503, -0.798662, -0.448886, 0.430222, -0.397201, 0.101137, -0.284337, -0.480231, 0.462471, -0.446348, 0.509206, 0.019989, -0.057749, 0.208735, -0.016589, 0.161241, 0.703013, 0.776739, 0.0424, -0.176884, -0.674657, -0.000357, 0.42098, -0.11259, -0.508509, 0.26214, 0.45593, -0.538032, -0.150992, 0.578031, -0.494937, -1.079834, 0.105698, -0.186454, 0.508847, 0.318936, 0.406802, 0.297553, -0.296594, 0.114135, -0.576564, 0.147387, -0.087478, -0.316172, -0.761727, 0.296877, 0.825913, 0.612421, -0.497998, 0.433574, 0.099738, 0.288592, -0.166623, 0.532389, 1.366488, 0.254006, -0.097321, 0.232047, -0.645755, 0.177315, -0.203091, -0.486085, 1.004249]\n",
      "\n",
      "First 30 vocabulary words: ['movie', 'wa', 'film', 'one', 'make', 'see', 'like', 'get', 'ha', 'good', 'watch', 'go', 'time', 'character', 'even', 'would', 'think', 'story', 'really', 'scene']\n"
     ]
    }
   ],
   "source": [
    "# 1. Let's see the word embedding for \"apple\" by passing in \"apple\" as the key.\n",
    "print(\"Embedding for apple:\", glove_wv[\"movie\"])\n",
    "\n",
    "# 2. Inspect the model vocabulary by accessing keys of the \"wv.vocab\" attribute. We'll print the first 20 words.\n",
    "print(\"\\nFirst 30 vocabulary words:\", list(glove_wv.keys())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha y Hora final: 2024-09-19 12:26:24.778966\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "fecha_hora_actual_final = datetime.now()\n",
    "print(\"Fecha y Hora final:\", fecha_hora_actual_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
