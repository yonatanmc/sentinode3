{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from node2vec import Node2Vec\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Cargar el dataset CSV\n",
    "df = pd.read_csv('/home/ymamani/projects/code/experimentos2/imdb_ds_2k_clean.csv')  # Reemplaza 'tu_dataset.csv' por el nombre de tu archivo\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "print(df.head())\n",
    "\n",
    "# Preprocesamiento de texto\n",
    "def preprocess_text(text):\n",
    "    # Tokenización\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Eliminar stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    # Unir tokens\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['clean_text'] = df['sw_text'].apply(preprocess_text)\n",
    "\n",
    "# Codificar etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['clean_text']\n",
    "y = df['label']\n",
    "\n",
    "# División en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_all_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Convertir a array\n",
    "X_all_tfidf_array = X_all_tfidf.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear grafo kNN\n",
    "k = 5  # Ajusta el número de vecinos según sea necesario\n",
    "A = kneighbors_graph(X_all_tfidf_array, n_neighbors=k, mode='connectivity', include_self=False)\n",
    "\n",
    "# Crear el grafo usando from_scipy_sparse_array\n",
    "G = nx.from_scipy_sparse_array(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Aplicar Node2Vec\n",
    "node2vec = Node2Vec(G, dimensions=300, walk_length=40, num_walks=300, workers=30)\n",
    "model = node2vec.fit(window=5, min_count=1, batch_words=4)\n",
    "\n",
    "# Obtener embeddings\n",
    "embeddings = np.array([model.wv[str(node)] for node in G.nodes()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapear índices de entrenamiento y prueba\n",
    "train_indices = X_train.index.values\n",
    "test_indices = X_test.index.values\n",
    "\n",
    "X_train_embeddings = embeddings[train_indices]\n",
    "X_test_embeddings = embeddings[test_indices]\n",
    "y_train_array = y_train.values\n",
    "y_test_array = y_test.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir clasificadores\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'SVM': SVC(probability=True),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Regresión Logística': LogisticRegression(max_iter=1000)\n",
    "}\n",
    "\n",
    "# Entrenar y evaluar cada clasificador\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train_embeddings, y_train_array)\n",
    "    y_pred = clf.predict(X_test_embeddings)\n",
    "    acc = accuracy_score(y_test_array, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test_array, y_pred, average='binary')\n",
    "    print(f'Clasificador: {name}')\n",
    "    print(f'Exactitud: {acc:.4f}')\n",
    "    print(f'Precisión: {precision:.4f}')\n",
    "    print(f'Exhaustividad (Recall): {recall:.4f}')\n",
    "    print(f'F1-score: {f1:.4f}')\n",
    "    print('----------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir datos a tensores de PyTorch\n",
    "x = torch.tensor(X_all_tfidf_array, dtype=torch.float)\n",
    "edge_index = torch.tensor(np.array([A.nonzero()[0], A.nonzero()[1]]), dtype=torch.long)\n",
    "y_tensor = torch.tensor(df['label'].values, dtype=torch.long)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y_tensor)\n",
    "\n",
    "# Crear máscaras de entrenamiento y prueba\n",
    "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "train_mask[train_indices] = True\n",
    "test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "test_mask[test_indices] = True\n",
    "data.train_mask = train_mask\n",
    "data.test_mask = test_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelo GCN\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, 64)\n",
    "        self.conv2 = GCNConv(64, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Entrenar modelo GCN\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_gcn = GCN(num_features=x.shape[1], num_classes=2).to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model_gcn.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train_gcn():\n",
    "    model_gcn.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model_gcn(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test_gcn():\n",
    "    model_gcn.eval()\n",
    "    logits = model_gcn(data)\n",
    "    pred = logits.argmax(dim=1)\n",
    "    y_true = data.y[data.test_mask].cpu()\n",
    "    y_pred = pred[data.test_mask].cpu()\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    return acc, precision, recall, f1\n",
    "\n",
    "# Entrenamiento\n",
    "for epoch in range(1, 200):\n",
    "    loss = train_gcn()\n",
    "    if epoch % 5 == 0:\n",
    "        acc, precision, recall, f1 = test_gcn()\n",
    "        print(f'Epoch: {epoch}, Loss: {loss:.4f}, Acc: {acc:.4f}, F1: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelo GAT\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(num_features, 8, heads=8, dropout=0.6)\n",
    "        self.conv2 = GATConv(8 * 8, num_classes, heads=1, concat=False, dropout=0.6)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Entrenar modelo GAT\n",
    "model_gat = GAT(num_features=x.shape[1], num_classes=2).to(device)\n",
    "optimizer_gat = torch.optim.Adam(model_gat.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "def train_gat():\n",
    "    model_gat.train()\n",
    "    optimizer_gat.zero_grad()\n",
    "    out = model_gat(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer_gat.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test_gat():\n",
    "    model_gat.eval()\n",
    "    logits = model_gat(data)\n",
    "    pred = logits.argmax(dim=1)\n",
    "    y_true = data.y[data.test_mask].cpu()\n",
    "    y_pred = pred[data.test_mask].cpu()\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    return acc, precision, recall, f1\n",
    "\n",
    "# Entrenamiento\n",
    "for epoch in range(1, 200):\n",
    "    loss = train_gat()\n",
    "    if epoch % 5 == 0:\n",
    "        acc, precision, recall, f1 = test_gat()\n",
    "        print(f'Epoch: {epoch}, Loss: {loss:.4f}, Acc: {acc:.4f}, F1: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener embeddings de GCN\n",
    "model_gcn.eval()\n",
    "with torch.no_grad():\n",
    "    x_gcn = model_gcn.conv1(data.x, data.edge_index)\n",
    "    x_gcn = F.relu(x_gcn)\n",
    "    embeddings_gcn = x_gcn.cpu().numpy()\n",
    "\n",
    "# Obtener embeddings de GAT\n",
    "model_gat.eval()\n",
    "with torch.no_grad():\n",
    "    x_gat = model_gat.conv1(data.x, data.edge_index)\n",
    "    x_gat = F.elu(x_gat)\n",
    "    embeddings_gat = x_gat.cpu().numpy()\n",
    "\n",
    "# Dividir embeddings en entrenamiento y prueba\n",
    "X_train_embeddings_gcn = embeddings_gcn[train_indices]\n",
    "X_test_embeddings_gcn = embeddings_gcn[test_indices]\n",
    "\n",
    "X_train_embeddings_gat = embeddings_gat[train_indices]\n",
    "X_test_embeddings_gat = embeddings_gat[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para entrenar y evaluar clasificadores con embeddings proporcionados\n",
    "def evaluate_classifiers(X_train_emb, X_test_emb, y_train, y_test, embedding_name):\n",
    "    print(f'\\n--- Resultados usando embeddings de {embedding_name} ---')\n",
    "    for name, clf in classifiers.items():\n",
    "        clf.fit(X_train_emb, y_train)\n",
    "        y_pred = clf.predict(X_test_emb)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "        print(f'Clasificador: {name}')\n",
    "        print(f'Exactitud: {acc:.4f}')\n",
    "        print(f'Precisión: {precision:.4f}')\n",
    "        print(f'Exhaustividad (Recall): {recall:.4f}')\n",
    "        print(f'F1-score: {f1:.4f}')\n",
    "        print('----------------------------------------')\n",
    "\n",
    "# Evaluar clasificadores con embeddings de GCN\n",
    "evaluate_classifiers(X_train_embeddings_gcn, X_test_embeddings_gcn, y_train_array, y_test_array, 'GCN')\n",
    "\n",
    "# Evaluar clasificadores con embeddings de GAT\n",
    "evaluate_classifiers(X_train_embeddings_gat, X_test_embeddings_gat, y_train_array, y_test_array, 'GAT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
